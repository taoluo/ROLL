defaults:
  - ../config/envs@_here_
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

exp_name: "agentic_pipeline"
seed: 42
logging_dir: ./output/logs
output_dir: ./output
render_save_dir: ./output/render
system_envs:
  USE_MODELSCOPE: '1'

#track_with: wandb
#tracker_kwargs:
#  api_key:
#  project: roll-agentic
#  name: ${exp_name}_sokoban
#  notes: "agentic_pipeline"
#  tags:
#    - agentic
#    - roll
#    - baseline

#track_with: tensorboard
#tracker_kwargs:
#  log_dir: /data/oss_bucket_0/yali/llm/tensorboard/roll_exp/agentic_sokoban

track_with: ml_tracker
tracker_kwargs:
 project: roll_pipeline_exp # ml_tracker的实验名，
 notes: "scale aligner pipeline"
 tags: # mltracker job tags，后续方便管理实验
   - pipeline
   - roll

checkpoint_config:
  type: file_system
  output_dir: /data/cpfs_0/rl_examples/models/${exp_name}

num_gpus_per_node: 8

max_steps: 1024
save_steps: 10000
logging_steps: 1
eval_steps: 10
resume_from_checkpoint: false

rollout_batch_size: 1024
val_batch_size: 1024
sequence_length: 8192

advantage_clip: 0.2
ppo_epochs: 1
adv_estimator: "grpo"
#pg_clip: 0.1
#dual_clip_loss: True
init_kl_coef: 0.0
whiten_advantages: true
entropy_loss_coef: 0
max_grad_norm: 1.0

pretrain: Qwen/Qwen2.5-0.5B-Instruct
reward_pretrain: Qwen/Qwen2.5-0.5B-Instruct

actor_train:
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
  training_args:
    learning_rate: 1.0e-6
    weight_decay: 0
    per_device_train_batch_size: 2
    gradient_accumulation_steps: 64
    warmup_steps: 10
    lr_scheduler_type: cosine
  data_args:
    template: qwen2_5
  strategy_args:
#    strategy_name: deepspeed_train
#    strategy_config: ${deepspeed_zero3}
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      use_distributed_optimizer: true
      recompute_granularity: full
  device_mapping: list(range(0,8))
  infer_batch_size: 2

actor_infer:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: 128 # single-turn response length
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.99
    num_return_sequences: 1
  data_args:
    template: qwen2_5
  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.8
      block_size: 16
      load_format: auto
  device_mapping: list(range(0,8))

reference:
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  data_args:
    template: qwen2_5
  strategy_args:
    strategy_name: hf_infer
    strategy_config: ~
  device_mapping: list(range(0,8))
  infer_batch_size: 2


action_pattern: <answer>(.*?)</answer>
think_action_pattern: <think>(.*?)</think>\s*<answer>(.*?)</answer>
user_prompt_no_think_format: <answer> [your answer] </answer>
user_prompt_think_format: <think> [Your thoughts] </think> <answer> [your answer] </answer>
added_text_no_think: <answer>
added_text_think: <think>

max_tokens_per_step: 128
max_actions_per_traj: 10

reward_normalization:
  grouping: traj_group_id # 可以tags(env_type)/traj_group_id(group)/batch(rollout_batch)... group_by计算reward/adv
  method: mean_std # asym_clip / identity / mean_std

train_env_manager:
  format_penalty: -0.15 # sokoban env penalty_for_step=-0.1
  max_env_num_per_worker: 16
  num_env_groups: 128
  # under the same group, the env config and env seed are ensured to be equal
  group_size: 8
  tags: [SimpleSokoban]
  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation

val_env_manager:
  max_env_num_per_worker: 32
  num_env_groups: 1024
  group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
  tags: [SimpleSokoban, LargerSokoban, SokobanDifferentGridVocab, FrozenLake]
  num_groups_partition: [256, 256, 256, 256] # TODO: If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation


custom_envs:
  SimpleSokoban:
    env_type: sokoban
    max_tokens_per_step: ${max_tokens_per_step}
    user_prompt_format: ${user_prompt_no_think_format}
    added_text: ${added_text_no_think}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager
    use_thread_lock: true
    env_config: # keys should be a subset of SokobanConfig
      env_instruction: "You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>"
      action_pattern: ${action_pattern}
      max_steps: ${max_actions_per_traj}
      dim_x: 6
      dim_y: 6
      num_boxes: 1
  LargerSokoban:
    env_type: sokoban
    max_tokens_per_step: ${max_tokens_per_step}
    user_prompt_format: ${user_prompt_no_think_format}
    added_text: ${added_text_no_think}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager
    use_thread_lock: true
    env_config:
      env_instruction: "You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>"
      action_pattern: ${action_pattern}
      max_steps: ${max_actions_per_traj}
      dim_x: 8
      dim_y: 8
      num_boxes: 2
      search_depth: 10
  SokobanDifferentGridVocab:
    env_type: sokoban
    max_tokens_per_step: ${max_tokens_per_step}
    user_prompt_format: ${user_prompt_no_think_format}
    added_text: ${added_text_no_think}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager
    use_thread_lock: true
    env_config: # keys should be a subset of SokobanConfig
      env_instruction: "You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>"
      action_pattern: ${action_pattern}
      max_steps: ${max_actions_per_traj}
      search_depth: 30
      dim_x: 6
      dim_y: 6
      num_boxes: 1
      grid_lookup: { 0: "W", 1: ".", 2: "G", 3: "C", 4: "B", 5: "A", 6: "@" }
      grid_vocab: { "W": "wall", ".": "empty", "G": "target", "C": "box on target", "B": "box", "A": "player", "@": "player on target" }
  FrozenLake:
    env_type: frozen_lake
    max_tokens_per_step: ${max_tokens_per_step}
    user_prompt_format: ${user_prompt_no_think_format}
    added_text: ${added_text_no_think}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager
    use_thread_lock: true
    env_config:
      env_instruction: "You are solving the FrozenLake puzzle. Forbid the whole and go to the target. You may move to the unintended direction due to the slippery ice. The answer must be one of action in a turn, format is <answer>Right</answer>"
      action_pattern: ${action_pattern}
      max_steps: ${max_actions_per_traj}
      is_slippery: false
  FrozenLakeThink:
    env_type: frozen_lake
    max_tokens_per_step: ${max_tokens_per_step}
    user_prompt_format: ${user_prompt_think_format}
    added_text: ${added_text_think}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager.TrajEnvManager
    use_thread_lock: true
    env_config:
      env_instruction: "You are solving the FrozenLake puzzle. Forbid the whole and go to the target. You may move to the unintended direction due to the slippery ice. The answer must be one of action in a turn, format is <answer>Right</answer>"
      action_pattern: ${think_action_pattern}
      max_steps: ${max_actions_per_traj}
      is_slippery: false
